---
title: "Top_2023_Fashion_Trends"
author: "Eva Nikolai"
date: "2023-03-03"
output: md_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Top 2023 Fashion Trends

As fashion weeks are underway, it is always fun to get an idea of what trends we will see hitting the runways. 

To get a better idea of key trends consumers can expect to see more frequently from designers, I decided to run a topic model on the top 10 results of 2023 fashion trends. 

Load in necessary libraries:
```{r}
library(rvest)
library(stringr)
```


To start, I scraped in 10 different fashion articles.

**Refinery29:**
```{r}
refinery <- read_html("https://www.refinery29.com/en-us/fashion-trends-2023")|> 
  html_elements("h2") |> html_text()

## clean refinery

refinery1 <- gsub("2023", "", refinery)

refinery2 <- gsub("Fashion\\sTrend", "", refinery1)

refinery3 <- gsub("Refinery.*", "", refinery2)
```


**Hello!:**
```{r}
hello <- read_html("https://www.hellomagazine.com/hfm/20221017154410/2023-fashion-trends-to-have-on-your-radar/")|>   html_elements("strong") |> html_text()

## clean Hello

hello1 <- gsub("\\s\\s+", "", hello)

hello2 <- gsub("SHOP\\sNOW", "", hello1)

hello3 <- gsub("^RELATED.*|^READ.*|^MORE.*|^Hello!.*|^More\\son.*|^HELLO.*", "", hello2)

hello4 <- gsub("^READ.*", "", hello3)

hello <- gsub("Spring\\s2023\\s[Tt]rends", "", hello4)

hello <- gsub("Summer\\s2023\\s[Tt]rends", "", hello)

hello <- hello[-c(64)]
```


**Vogue:**
```{r}
vogue <- read_html("https://www.vogue.com/article/spring-2023-trends-editors-picks")|> html_elements("h2") |> html_text()

## clean Vogue:

vogue1 <- gsub("^By\\s[A-Z].*", "", vogue)

vogue2 <- gsub("revist.*|Vogue.*|commerce.*|editor", "", vogue1)

vogue <- gsub("[Ss]ign\\sup.*|[Ss]igning\\sup.*", "", vogue2)
```


**Glamour:**
```{r}
glamour <- read_html("https://www.glamour.com/story/2023-style-trends") %>%  html_elements("strong") %>%  html_text

glamour1 <- gsub("^By\\s[A-Z].*|Courtesy.*", "", glamour)

glamour2 <- gsub("revisit.*", "", glamour1)

glamour <- gsub("2023\\sStyle\\sTrend:", "", glamour2)
```


**Insider:**
```{r}
insider <- read_html("https://www.insider.com/fashion-clothing-trends-coming-this-year-2023#lug-sole-loafers-will-remain-a-strong-trend-1") %>% html_elements("h2") %>% html_text
```


**Elle:**
```{r}
elle <- read_html("https://www.elle.com/fashion/trend-reports/a41340278/spring-2023-fashion-trends/") %>% html_elements("h2") %>% html_text
```


**Woman and Home:**
```{r}
womanhome <- read_html("https://www.womanandhome.com/fashion/fashion-trends-2023/") %>% html_elements("span") %>% html_text

womanhome1 <- gsub("opens.*", "", womanhome)

wh2 <- gsub("Sign.*", "", womanhome1)

wh3 <- gsub("[(]", "", wh2)

wh4 <- gsub("^\\d+\\.\\s", "", wh3)

womanhome <- wh4[-c(1:41)]

womanhome <- womanhome[-c(184:219)]
```


**Forbes:**
```{r}
forbes <- read_html("https://www.forbes.com/sites/sboyd/2023/01/28/the-9-fashion-trends-youre-about-to-see-everywhere-in-2023/?sh=11d4fb25b05d") %>% html_elements("h2") %>% html_text

forbes <- gsub("^\\d+\\)\\s", "", forbes)
```


**Glamour UK:**
```{r}
glamour_UK <- read_html("https://www.glamourmagazine.co.uk/gallery/spring-summer-2023-fashion-trends") %>% html_elements("span") %>% html_text

glamour_UK1 <- glamour_UK[-c(92:131)]
  
glamour_UK1 <- gsub("^By\\s.*", "", glamour_UK1)

glamour_UK1 <- glamour_UK1[-c(1:36)]

glamour_UK <- gsub("([A-Z][a-z]+).*", "", glamour_UK1)
```


```{r}
bazaar <- read_html("https://www.harpersbazaar.com/fashion/trends/a41247745/spring-2023-fashion-trends/") %>% html_elements("p") %>% html_text

bazaar1 <- bazaar[-c(1:2)]

bazaar <- bazaar1[-c(7:24)]
```


**Combine articles:**
```{r}
all_articles <- c(hello, vogue, glamour, insider, elle, womanhome, forbes, glamour_UK, bazaar)

all_articles2 <- gsub("-", "", all_articles)
```


Load in necessary libraries:
```{r}
library(dplyr)
library(quanteda)
library(stm)
library(tm)
library(textstem)
```


**Text cleaning & prep of combined articles**
```{r}
articles_lowered <- tolower(all_articles2)

df <- data.frame(articles_lowered)

articles_df <- df[!apply(df==""|df==" ", 1, all),]

articles_df <- articles_df[-c(5:6)]

articles_df <- gsub("[Ff]ashion|[Tt]rend|[Tt]rends|[Pp]aris|[Ss]pring|[Ss]tory|[Rr]unway|[Rr]unways|[Ll]ook|[Ll]ooks", "", articles_df)

articles_df <- gsub("([[:punct:]])", "", articles_df)

articles_df <- gsub("[Nn]ew\\s[Yy]ork|[Ww]eek|[Ss]eason", "", articles_df)

removed_words <- tm::removeWords(tolower(articles_df), words = stopwords("en"))


lemma_dictionary <- make_lemma_dictionary(removed_words,   
                                          engine = 'hunspell')

articles_lemmatized <- lemmatize_strings(removed_words, 
                                          lemma_dictionary)
```


**Create the corpus with document variables**
```{r}
fashion_corpus <- corpus(articles_lemmatized)

fashion_token <- quanteda::tokens(fashion_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)

fashion_dfm <- dfm(fashion_token)

fashion_dfm <- dfm_trim(fashion_dfm, sparsity = 0.990)

fashion_stm <- convert(fashion_dfm, to = "stm")

docs_stm <- fashion_stm$documents 
vocab_stm <- fashion_stm$vocab    
meta_stm <- fashion_stm$meta

fashionPrep <- prepDocuments(documents = docs_stm, 
                           vocab = vocab_stm,
                           meta = meta_stm)
```


**Evaluate Term Frequency**

Out of curiosity, let's see what the most frequent terms are in each article:
```{r}
term_frequency <- tm::termFreq(articles_lemmatized)

head(sort(term_frequency, decreasing = TRUE),20)
```

From conducting the term frequency, we see that skirts, dresses, leather, denim, and sheer are the most used terms. This indicates that all 10 fashion articles believe these clothing articles/ styles will be particularly relevant in 2023 trends.

Let's keep this in mind when evaluating topics of scraped fashion articles. 

Ideally, these results will be able to identify the most popular trends for the 2023 fashion year.

## Topic Model: 

**Perform kTest to choose topic count**
```{r}
kTest <- searchK(documents = fashionPrep$documents, 
             vocab = fashionPrep$vocab, 
             K = c(3, 4, 5, 10, 20), verbose = FALSE)

plot(kTest)
```

After viewing the results of the kTest, the results are fairly interesting. Specifically, it is interesting to see the semantic coherence be so high with 20 topics. As the ideal topic selection would be the number of topics with highest semantic coherence and lowest residuals, we see the model suggests to select 10 topics. However, for some variation, I also tested using the next best result: 3. 

**Option 1 - Test 10 Topics:**
```{r}
topics10 <- stm(documents = fashionPrep$documents, 
             vocab = fashionPrep$vocab, seed = 1001,
             K = 10, verbose = FALSE)
```

Topic proportions plot at 10 topics:
```{r}
plot(topics10)
```

See what emerges from each topic by evaluating high prob & FREX words
```{r}
top10_topic_list <- labelTopics(topics10)

print(top10_topic_list)
```


**Option 1 - Test 20 Topics:**
```{r}
topics20 <- stm(documents = fashionPrep$documents, 
             vocab = fashionPrep$vocab, seed = 1001,
             K = 20, verbose = FALSE)
```

Topic proportions plot at 20 topics:
```{r}
plot(topics20)
```

See what emerges from each topic by evaluating high prob & FREX words
```{r}
top20_topic_list <- labelTopics(topics20)

print(top20_topic_list)
```

**Option 2 - Test 4 Topics:**
```{r}
topics4 <- stm(documents = fashionPrep$documents, 
             vocab = fashionPrep$vocab, seed = 1001,
             K = 4, verbose = FALSE)
```

Topic proportions plot at 4 topics:
```{r}
plot(topics4)
```

See what emerges from each topic by evaluating high prob & FREX words
```{r}
top4_topic_list <- labelTopics(topics4)

print(top4_topic_list)
```



**Option 2 - Test 5 Topics:**
```{r}
topics5 <- stm(documents = fashionPrep$documents, 
             vocab = fashionPrep$vocab, seed = 1001,
             K = 5, verbose = FALSE)
```

Topic proportions plot at 5 topics:
```{r}
plot(topics5)
```

See what emerges from each topic by evaluating high prob & FREX words
```{r}
top5_topic_list <- labelTopics(topics5)

print(top5_topic_list)
```


When comparing both topic models results to one another, there are clear similarities. It seems that in 2023, consumers can anticipate to see popularity of a variation of skirts and dresses, all utilizing many different fabrics. Specifically, we will see fashion retailers producing many crochet/ fringe looks, using sheer design elements, silk and leather fabrics, and the resurgence of denim and jean. 

Thank you for your interest in my analysis of the most relevant 2023 anticipated Fashion trends.